\documentclass[10tma4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{/home/stewryan/Documents/CPP/FET/Report/} }
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\title{Project Report}
\author{Ryan Stewart}
\begin{document}
	\begin{center}
		Implementation of Functional Expansion Tallies in the 
		Monte Carlo Code SHIFT
		\\
		\bigskip
		\small{Ryan H. Stewart, Leslie} Kerby
		\\
	\end{center}
	
	\begin{center}
		\textit{\small{Idaho State University, Department of Nuclear Engineering and Health Physics, Pocatello, ID 83209, stewryan@isu.edu}}
	\end{center}
	

\section{Abstract}\label{abstract}



\section{Introduction}\label{intro}


 Shift is a new Monte Carlo (MC) transport code developed by Oak Ridge National Laboratory for large scale reactor analysis and optimized to perform MC transport calculations on current and near-future computing architectures, while retaining the ability to be run on small clusters and personal laptops. The design of Shift allows for modular and easily extensible implementation of features such as physics and source implementations, hybrid capabilities with deterministic codes, and parallel decomposition. Implementation of these features allows for full utilization with large computing clusters to examine whole core reactor analysis with the hope of coupling Shift with computational fluid dynamics codes such as Nek5000. Shift utilizes traditional MC bin tallying techniques to determine collision rates or flux values within the core which can lead to excessive particle requirements and run times.
\\
Traditional MC tallies utilize two different classes of estimator, discrete and track length, to extract useful information during the MC process. The information is then separated into bins to create a histogram based solution for the underlying physically meaningful result such as flux, or reaction rate. These bins require many particles to provide meaningful results, and if a small bin is required a large variance may occur due to too few particles. To obtain reliable results the tally would have to be increased, thereby defeating the purpose, or the number of particle histories would need to be increased, which would increase the run time.
\\
An attempt to address these issues using functional expansion tallies (FETs) was implemented by Griesheimer in the code MCNPX. FETs use a set of basis functions, which form complete set, to estimate the shape of the flux (or current) as a series expansion. This provides a continuous representation of the flux compared to the bin method of traditional MC tallies. FETs also have the ability to extract higher order information about the current shape from particle histories, while retaining the lower order information, such as integral quantities. Along with this, FETs are not dependent on mesh or finite element calculations which provides a method to transfer data between multiple codes with differing geometric definitions.  
\\
To determine the applicability of FETs for Shift, only discrete estimators, in the form of surface currant tallies, were implemented. Legendre polynomials were utilized as the set of basis functions.

\section{Background}\label{background}

\subsection{Surface Current FET}\label{Surface Current FET}

FETs rely on a series expansion to approximate the shape of an unknown distribution. The series used for expansion can vary depending on the boundary conditions, geometry and prior of the distribution. Often times, there is little knowledge of a priori information and it is best to select a basis function such as the Legendre or Chebyshev Polynomials, which are known to have fast convergence rates for nearly smooth functions. For the work in Shift, Legendre Polynomials were selected for implementation.
\\
To first step in implementing FETs into Shift was the implementation of surface current. The surface current is easy to implement and can expressed as a series expansion, $J_{m}$ (x), although this can be generalized to multiple dimensions, collision types, energy levels, etc. The finite expansion of $J_{m}$ (x) can be described by
	\begin{equation} \label{eq:current approx}
	J_{m}(x) \approx \sum_{m=0}^{M}a_{m}\psi_{m}(x)k_{m}\rho(x)
	\end{equation}
Where,$k_{n}$ is the orthonormalization constant and is described by
\begin{equation}\label{eq:ortho const}
k_{n} = \frac{1}{||\psi(x)||}
\end{equation} 
This allows the individual coefficients to be described as
	 \begin{equation} \label{eq:a bar m}
	\bar{a}_{m}=\int j(x)\psi_{m}(x)\rho(x)dx
	 \end{equation}
In this sense, $a_m$ is the $m^{th}$ coefficient of the $m^{th}$ basis function, $\psi_{m}(x)$ (in this case, the Legendre Polynomial). For ease of implementation, it is often easiest to utilize a basis function with an orthogonal basis. Along with this, $\rho(x)$ is the associated weighing function for the basis function used. In the case of Legendre Polynomials, $\rho(x)=1$. Monte Carlo does not utilize an infinite number of particles, and thus an estimate of $a_{m}$ is required, namely
	 \begin{equation} \label{eq:a hat m}
	 \hat{a}_{m}=\frac{1}{N}\sum_{n=1}^{N}w_{n}\psi_{m}(x_{n})
	 \end{equation}
Where in the context of a discrete estimator; $w_{n}$ is the weight of the $n^{th}$ particle, and $\psi_{m}(x_{n})$ is the contribution of the $n^{th}$ particle to the $m^{th}$ order Legendre Polynomial $P_{n}(x)$, and N is the total number of particles that contributed to the coefficient. 

\subsection{Uncertainty in the FET}\label{UIC}

The Monte Carlo technique incorporates a random component, which causes an intrinsic statistical uncertainty associated with it. The functional expansion tallies are no exception to this and each coefficient will have an uncertainty associated with it. Along with this, the final expression of the current will have an uncertainty. The variance for each coefficient, $a_{m}$ is found using a derivation of the sum of squares law and is found to be 
	\begin{equation} \label{eq:coeff unc}
	\sigma_{\hat{a}_{m}}^2 = \frac{\sum_{n=1}^{N}[\sum_{m=0}^{M}w_{m,n}\psi_{m}(x)]^{2} - \frac{1}{N}[ \sum_{n=1}^{N}\sum_{m=0}^{M}w_{m,n}\psi_{m}(x)]^{2}}{N(N-1)}
	\end{equation}
This uncertainty is an estimator of the real uncertainty, which requires prior knowledge of the probability density function. This knowledge is rarely known, and thus a statistical estimator is required.
\\With the coefficients of the basis function found, along with their associated uncertainties, the Monte Carlo approximation of $\phi(x)$ can be written as
	\begin{equation} \label{eq:current hat}
	\hat{J}(x) = \sum_{m=0}^{M} \hat{a}_{m}k_{m}\psi_{m}(x)
	\end{equation}
As with the coefficients, Monte Carlo approximations are meaningless without the corresponding variance, which is given by the two-norm variances
	\begin{equation} \label{eq:current unc}
	\sigma_{\hat{J}(x)}^2=\sum_{m=0}^{M}\sigma_{\hat{a}_{m}}^2k_{m}
	\end{equation}
Where $a_{m}^{n}$ is the contribution of particle n to the $m^{th}$ coefficient.

\subsection{Truncation Error \& Optimization in FETs}\label{TEO}

The current in equation \eqref{eq:current approx} is only exact if an infinite number of coefficients are calculated. An infinite number of coefficients is not possible when utilizing a Monte Carlo system, thus the coefficients must be truncated at some value, which introduces truncation error in the current. The truncation error, $E_{m}(x)$ introduced is equal to the value of all the expansion terms that are greater than truncated coefficient m.
\begin{equation}\label{eq:trunc unc}
E_{m}(x) = |J(x)-J_{m}(x)|=|\sum_{n=M+1}^{\infty}\hat{a}_{m}k_{n}\psi_{n}(x)|
\end{equation}

Thus for each coefficient, $\hat{a}_{n}$, added, the truncation error is decreased by a factor of $\hat{a}_{n}^2$. However, each time a new coefficient is added the statistical error is increased due to the new coefficients error being added. The increase in statistical error is given by $\hat{\sigma}_{\hat{a}_{n}}^2k_{n}$, or variance associated with the $\hat{a}_{n}$ term. It is seen that the two terms are inversely related, and thus an optimization can be found to relate the two quantities and determine reliability. This optimization can be found in the ratio between the statistical error and the trucnation error, given as
\begin{equation}\label{eq:ratio}
R_{n}^2=\frac{\hat{\sigma}_{\hat{a}_{n}}^2k_{n}}{\hat{a}_{n}^2}
\end{equation}
Equation \eqref{eq:ratio} gives a relative cost-to-benefit ratio to including the nth term of the series. This ratio can be used as a test to determine if the nth expansion coefficient should be included. Where an $R_{n}^2>>1$ indicates that the coefficient has not converged on the true solution and will yield poor results for the function approximation. $R_{n}^2<<1$ indicates that the coefficient has converged on the solution and will yield good results for the function. $R_{n}^2\approx1$ indicates that the coefficient should be examined carefully before including the term for the function approximation. A value of $R_{n}^2\approx1$ is indicative that more particles could be run to allow the coefficient including in the function approximation.


\section{Algorithm}\label{algorith}
\subsection{One Dimensional}\label{alg oned}
The algorithm for solving the FET's was written in C++ to allow for an ease in integrating into SHIFT, which is also written in C++. To solve for the surface current, data had to be obtained from the source code in SHIFT and transfered to the FET algorithm. The FET algorithm required the particle weight and the surface to be tallied. With this information, an approximation could be made for current shape with a functional expansion tally.
\\
The FET algorithm was simple in nature to allow for a more efficient method in calculating tallies. The first step was to obtain the data from SHIFT that would be required. This involved activating the algorithm each time a tally surface was crossed. When a tally surface was crossed, the weight of the particle and the tally surface number was transfered into the functional expansion tally algorithm. The surface is bound by two maxima and minima which is used to transfer the particles position from the surface space to Legendre space. This is done using the following equation, where $\tilde{x}$ is the Legendre space transform of position $x$ in surface space:
\begin{equation} \label{eq:legendre transform}
\tilde{x} = 2 \frac{x-x_{min}}{x_{max}-x_{min}} - 1
\end{equation}
The transformed $\tilde{x}$ is then plugged into the Legendre polynomial and solved. Each coefficient for the Legendre polynomial is solved for and a vector containing the coefficients is produced. This process can be repeated multiple times depending on the number of times the particle crosses tally surface. Each time a particle crosses the tally surface, the Legendre coefficients are summed to create $a_{n}$. Once a particle is killed, the $a_{n}$'s are summed to create an estimate of $\hat{a}_{m}$, designated as $A_{n}$. In addition to this, for each $a_{n}$, a separate sum of $a_{n}^{2}$ is kept, and designated as $A_{m}$. The value of $A_{m}$ is used post processing for calculating the variance. Along with this, a running tally for the number of particles crossing the surface of interest is kept for post processing.
\\
Once the simulation is complete, and all of the histories/particles have been run, post processing  is performed to find the values of $\hat{a}_{n}$, their associate variance, the current, and the overall variance through the surface. To find $\hat{a}_{m}$, the final $A_{n}$ is divided by the total number of particles crossing the surface of interest. This normalizes the coefficients according to the particle fluence through the surface. \\
To find the variance, equation \eqref{eq:coeff unc} was used where $[\sum_{m=0}^{M}w_{m,n}\psi_{m}(x)]^{2}$ is represented by $A_{m}$, and $\sum_{n=1}^{N}\sum_{m=0}^{M}w_{m,n}\psi_{m}(x)$ is represented by $A_{n}$. Once the variance is found per coefficient, the current can be written as
\begin{equation}\label{eq:alg current}
	\tilde{J_{m}}(\tilde{x}) = \sum_{m=0}^{M}\hat{a}_{m}k_{m}P_{m}(\tilde{x}) \pm  \sigma_{\hat{a}_{m}}
\end{equation}
It should be noted that equation \eqref{eq:alg current} is still contained in Legendre space, that is [-1,1]. If the user desired to transform the equation to original units, a simple transformation of $\tilde{x}$ is required, as in equation \eqref{eq:legendre retrans}.
\begin{equation}\label{eq:legendre retrans}
\tilde{x} = 2\frac{x-x_{min}}{x_{max}-x_{min}}-1
\end{equation}
Where $\tilde{x}$ from equation \eqref{eq:alg current} is replaced with equation \eqref{eq:legendre retrans}. Along with this, the orthonormalization constant must be transformed from $k'_{m}$ back to $k_{m}$ via equation \eqref{eq:ortho const re}
\begin{equation}\label{eq:ortho const re}
k'_{n} = \frac{2n+1}{x_{max}-x_{min}}
\end{equation}
With the transformations from equations \eqref{eq:legendre retrans} and \eqref{eq:ortho const re} the current for the original tally domain is described as 
\begin{equation}\label{eq:final current}
	\hat{J_{m}}(x) = \sum_{m=0}^{M}\hat{a}_{m}k'_{m}P_{m}(2\frac{x-x_{min}}{x_{max}-x_{min}}-1) \pm  \sigma_{\hat{a}_{m}}
\end{equation}
Again, if multiple dimensions were to be used, each transformation can be performed independently and multiplied together to yield a final current. 

\subsection{Two Dimensional}\label{alg twod}
The two (or multi) dimensional algorithm is identical to the one dimensional algorithm, where each dimension is solved separately. If both an x, and y directions are required, then a separate $A_{n,x}$ and $A_{n,y}$ are required, and similarly a separate $A_{m,y}$ and $A_{m,y}$ are required for the uncertainty.

\section{Verification \& Validation}\label{vv}
\subsection{Verification of Functionality}\label{P1}

The first phase of the verification process was to ensure the algorithm was performing the intended calculation and producing reliable results. To test this, a test function was developed. The test function used rejection sampling to sample the shape of a user defined distribution. For the sake of testing, multiple distributions were selected to determine accurate results, and to learn additional nuances of FET properties; such as time requirements, optimal coefficient selection, particle number, and statistical uncertainties.
\\
For the first test, a distribution of $5x^{3}-x^{2}-2x+4$ was used. This initial test was to determine if the algorithm created was performing its intended function and creating a Legendre polynomial that matched the distribution function. The results were promising, for nearly any number of particles, as seen in Table \ref{table:alg res}. For the results in Table \ref{table:alg res}, seven coefficients were used which can be seen, along with their corresponding uncertainties in Table \ref{table:coef res}. Along with these, the uncertainty in each coefficient can be seen in Table \ref{table:coef unc res}.
\begin{table}[!htbp]
	\caption{FET Algorithm Results}
	\centering
	\begin{tabular}{c c c c c c c c}
		\hline
		Position & Distribution &  & FE &  & Diff. &  &\\[0.5ex]
		 & & 1e5 & 1e6 & 1e7 & 1e8 & 1e9 &\\
		 \hline
		 \hline
		-1.0 & 0.00 & -2.32e-2 & 1.91e-3  & -1.41e-3 & 1.63e-3  & 3.41e-4  & \\
		-0.6 & 3.76 & -2.94e-2 & 3.04e-3  & 3.94e-3  & 4.68e-4  & 1.96e-5  & \\
		-0.2 & 4.32 & -1.74e-2 & -5.40e-3 & -3.09e-3 & -6.71e-4 & 4.04e-5  & \\
		 0.2 & 3.60 & 1.98e-2  & -4.01e-3 & -1.26e-4 & -1.66e-4 & 5.83e-5  & \\
		 0.6 & 3.52 & 1.04e-2  & 6.93e-3  & -1.20e-3 & -2.88e-4 & -2.15e-4 & \\
		 1.0 & 6.00 & -7.93e-2 & -1.11e-2 & 1.90e-2  & 5.98e-3  & 2.254    & \\ [1ex]
		\hline
	\end{tabular}
	\label{table:alg res}
\end{table}
\begin{table}[!htbp]
	\caption{FET Coefficient Values}
	\centering
	\begin{tabular}{c c c c c c c }
		\hline
		Coefficient & &  & FE  &  & & \\[0.5ex]
		 & 1e5 & 1e6 & 1e7 & 1e8 & 1e9 &\\
		 \hline		
		\hline
		$P_{0}(x)$ & 3.67     & 3.67     & 3.67     & 3.67     & 3.67     & \\
		$P_{1}(x)$ & 0.99     & 1.00     & 1.00     & 1.00     & 1.00     & \\
		$P_{2}(x)$ & -0.66    & -0.67    & -0.67    & -0.67    & -0.67    & \\
		$P_{3}(x)$ & 2.05     & 2.00     & 2.00     & 2.00     & 2.00     & \\
		$P_{4}(x)$ & 3.10e-3  & 1.13e-2  & 2.35e-4  & -7.11e-4 & -2.31e-4 & \\
		$P_{5}(x)$ & -1.21e-2 & 1.92e-3  & -7.83e-3 & -1.36e-3 & -1.31e-4 & \\
		$P_{6}(x)$ & 3.74e-2  & -4.45e-3 & -6.51e-3 & -2.44e-3 & -2.14e-4 & \\ [1ex]
		\hline
	\end{tabular}
	\label{table:coef res}
\end{table}
\begin{table}[!htbp]
	\caption{FET Coefficient Uncertainty}
	\centering
	\begin{tabular}{c c c c c c c }
		\hline
		Coefficient & & & FE & & & \\[0.5ex]
		& 1e5 & 1e6 & 1e7 & 1e8 & 1e9 &\\
		\hline		
		\hline
		$P_{0}(x)$ & 3.14e-3 & 9.97e-4 & 3.15e-4 & 1.00e-4 & 3.15e-5 & \\
		$P_{1}(x)$ & 6.71e-3 & 2.12e-3 & 6.70e-4 & 2.12e-4 & 6.70e-5 & \\
		$P_{2}(x)$ & 5.46e-3 & 1.73e-3 & 5.45e-4 & 1.72e-4 & 5.45e-5 & \\
		$P_{3}(x)$ & 4.56e-3 & 1.44e-3 & 4.54e-4 & 1.44e-4 & 4.55e-5 & \\
		$P_{4}(x)$ & 4.07e-3 & 1.28e-3 & 4.06e-4 & 1.28e-4 & 4.06e-5 & \\
		$P_{5}(x)$ & 3.69e-3 & 1.16e-3 & 3.66e-4 & 1.16e-4 & 3.66e-5 & \\
		$P_{6}(x)$ & 3.39e-3 & 1.07e-3 & 3.37e-4 & 1.06e-4 & 3.37e-5 & \\ [1ex]
		\hline
	\end{tabular}
	\label{table:coef unc res}
\end{table}
\\
It can be seen from Tables \ref{table:alg res} - \ref{table:coef unc res} that increasing the number of particles decrease the difference between the real distribution and the functional expansion distribution. Along with this, an increase in the number of particles decreases the uncertainty for the individual coefficients. This provides confidence that the FET algorithm is performing its intended functions and calculating an accurate result with a test function. A graphical representation of this can be seen in Figure \ref{2D Plot}. Along with this, when the test function is represented in two dimensions, figure \ref{3D Plot} shows the functional expansion plot. Along with this, Figure \ref{3D Plot} shows the difference between the FET and the direct calculation of the test function. Due to this test function being cubic in order, the higher order coefficients remain small and unsteady, but congregate around zero. Thus, the higher order terms show a lower importance to the solution, and could be neglected if desired. This is purely a consequence of using a contrived test function, and not indicative of true Monte Carlo simulations. In a simulation regarding a neutron population passing through a surface, each coefficient value would converge towards a true value with an increased neutron population.
\\
\begin{figure}[!htbp]
	\caption{Comparison for number of particles FE to $5x^{3}-x^{2}-2x+4$}
	\begin{center}
	\includegraphics[scale=0.75]{2D_Ver_Plot}
	\label{2D Plot}
	\end{center}
\end{figure}
\begin{figure}[!htbp]
	\caption{3D FET \& Difference Map}
	\begin{center}
		\includegraphics[scale=0.38]{3D_Ver_Plot}
		\includegraphics[scale=0.38]{3D_Ver_Plot_Diff}
	\end{center}
	\label{3D Plot}
\end{figure}


\subsection{Verification in Shift}\label{P2}

\section{Optimization}\label{optimization}

Creating the algorithm to solve for FETs underwent multiple iterations to create a semi-optimized function which performs the calculations efficiently and accurately. The first iteration of the Legendre solver involved using a recursion only Legendre solver. To solve for this, the following recursion formula was used
\begin{equation}\label{recursion}
P_{n}(x) = \frac{(2*n-1)xP_{n-1}-(n-1)P_{n-2}}{n}
\end{equation}
Where $P_{0}(x)=1$ and $P_{1}(x)=x$. Using the recursion relation requires the relation to be called $\sum_{n=1}^{N}x$ times to solve for all the coefficients. Most FETs require 20 or fewer coefficients to converge on the solution, and thus a direct calculation approach can save both time and the number of calculations required. If 20 coefficients are used, then 2310 calculations are required per particle, using the recursion method. To minimize the number of calculations required, the first twelve solutions are solved for explicitly, where the remained are solved. Using the explicit solutions for the first twelve coefficients reduces the number of calculations by 21\%, down to 1817 calculations. Directly solving for the first twelve coefficients is not initially intuitive, but additional optimization provides a basis for choosing twelve coefficients.
\\
In addition to directly solving the Legendre coefficients for the first twelve coefficients, the Legendre solver function was optimized by vectorizing the coefficients. This process involved solving for the each coefficient required at during one pass. Initially, the Legendre solver function was called each time a coefficient was required. Thus, if 20 coefficients were required, then the Legendre solver function as called twenty times to obtain each coefficient. This prevented saving any of the data obtained from performing the function previously, which meant that every time the function was called, it had to recalculate all the coefficients, even if they had already been calculated. Vectorization instead created a vector out of the coefficients, which allowed the Legendre solver function to be called once, rather than multiple times. The vectorization process also allowed each coefficient that had previously been calculated to be saved, and used for any higher order functions. For example, if the 20th coefficient was being calculated using \eqref{recursion}, then $P_{n-1}$ and $P_{n-2}$ had already been calculated and those values could be plugged in and $P_{20}(x)$ could be solved for readily. This process dramatically decreased the number of calculations required to solve for the coefficients. If the example of 20 coefficients is used again, and no direct calculations are present then the number of calculations required is still 2310. If vectorization is utilized, and the values for solved Legendre functions are saved then only 210 calculations are required, which decreases the number of calculations by 90\%. If direct calculations are included, then only 159 calculations are required, which decreases the number of calculations by 93\%. This reduction in the number of computations required per particle helps greatly reduce the time to solve for the FETs in general. Returning to the number of direct calculations required, it was found that the number of calculations required for a the direct method was the coefficient number plus one. Thus around the tenth coefficient, it took nearly an as many calculations to perform a direct calculation as it does to calculate the recursion form. The reduction in the number of calculations required to solve fo the functional expansion can be seen in Table \ref{table:vect op}.

\begin{table}[htbp!]
	\caption{Vectorization \& Direct Calculation Optimization}
	\centering
	\begin{tabular}{c c c c c c c }
		\hline
		Method & & & Time (s) & & & \\[0.5ex]
		Number of Particles& 1e5 & 1e6 & 1e7 & 1e8 & 1e9 &\\
		\hline		
		\hline
		Recursion & 1.64 & 16.3 & 163 & 1640 & ~16400 & \\
		Direct + Vectorization & 0.213 & 2.13 & 21.6 & 213 & 2160 & \\[1ex]
		\hline
	\end{tabular}
	\label{table:vect op}
\end{table}

From Table \ref{table:vect op}, when both the direct calculation and vectorization are implemented the calculation time is reduced by 85\%. This reduction in time will be extremely valuable when a Monte Carlo problem requires multiple histories, each with mullions or billions of particles. If, the original recursion time was required, the FET would potentially be useless in the application of Exascale computing.
\\
Other miscellaneous optimization techniques steamed from the choice data storage. This included using multiple 2D matrix arrays to store different surfaces that required tallies. Multiple 2D matrices was found to be easier to manipulate and required less manipulations to be performed when accessing storage locations, than using one large 3D matrix. 

\section{Implementation into SHIFT}\label{shift}
Shift provides users with multiple ways to score tallies for a Monte Carlo simulation. Each tally type is accessed through a central class in Shift called Tallier. This class registers each type of tally that is going to be used in the simulation. As a first attempt to implement the functional expansion tally, it was placed inside the tally which counted surface crossings. 

\section{Conclusions}\label{conclusion}

\section{Further Research}\label{further}


\end{document}